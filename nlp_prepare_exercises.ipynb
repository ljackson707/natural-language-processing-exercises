{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "The end result of this exercise should be a file named prepare.py that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "    - Lowercase everything\n",
    "    - Normalize unicode characters\n",
    "    - Replace anything that is not a letter, number, whitespace or a single quote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(article):\n",
    "    '''\n",
    "    This function takes in a article in string format.\n",
    "    \n",
    "    Turns all letters into lowercase.\n",
    "    \n",
    "    Normalizes the unicode characters using the NFKD method,\n",
    "    while ignoring any unknow characters.\n",
    "    \n",
    "    Will replace anything that is NOT letters, numbers, whitespace or single quote.\n",
    "    \n",
    "    This funtion will return a basic cleaned article in string format\n",
    "    '''\n",
    "    \n",
    "    # Lowercase \n",
    "    article = article.lower()\n",
    "    \n",
    "    # Normalization\n",
    "    article = unicodedata.normalize('NFKD', article)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    \n",
    "    # Replace\n",
    "    article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Paul Erdős and George Pólya are influential Hungarian mathematicians who contributed a lot to\\\n",
    "the field. Erdős's name contains the Hungarian letter 'ő' ('o' with double acute accent), but is often\\\n",
    "incorrectly written as Erdos or Erdös either by mistake or out of typographical necessity\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot tothe field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is oftenincorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = basic_clean(article)\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(article):\n",
    "    '''\n",
    "    This function takes in an article as a string.\n",
    "    Creates a tokenizer using nltk.\n",
    "    Uses the tokenize on the artical and returns the article in string fromat.\n",
    "    '''\n",
    "    # Create the tokenizer\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    tokenizer.tokenize(article, return_str = True)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot tothe field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is oftenincorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = tokenize(article)\n",
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(article):\n",
    "    '''\n",
    "    This function takes in an article as a string.\n",
    "    Creates a porter stemmer.\n",
    "    Applies the stemmer to each word in the article/string.\n",
    "    Joins the stems into a single string called article_stemmed.\n",
    "    Returns article_stemmed with all stemed characters. \n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Apply the stemmer to each word in our string.\n",
    "    stems = [ps.stem(word) for word in article.split()]\n",
    "    \n",
    "    # Join stems\n",
    "    article_stemmed = ' '.join(stems)\n",
    "    \n",
    "    return article_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdo and georg polya are influenti hungarian mathematician who contribut a lot toth field erdos' name contain the hungarian letter 'o' 'o' with doubl acut accent but is oftenincorrectli written as erdo or erdo either by mistak or out of typograph necess\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_stemmed = stem(article)\n",
    "article_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(article):\n",
    "    '''\n",
    "    This function takes in an article as a string.\n",
    "    Downloads 'wordnet' from nltk.\n",
    "    Creates a lemmatizer.\n",
    "    Applies the lemmatizer to each word in the article/string.\n",
    "    Joins the lemmmas into a single string called article_lemmatized.\n",
    "    Returns artical_lemmatized with all lemmatized characters. \n",
    "    '''\n",
    "    # Download the first time.\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    # Create the Lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Apply the lemmatize to each word in our string.\n",
    "    lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "    \n",
    "    # Join lemmas\n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    \n",
    "    return article_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/liamjackson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematician who contributed a lot tothe field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is oftenincorrectly written a erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_lemmatized = lemmatize(article)\n",
    "article_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "   - This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words = 'my'\n",
    "exclude_words = 'are'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos george polya influential hungarian mathematicians contributed lot tothe field erdos's name contains hungarian letter 'o' 'o' double acute accent oftenincorrectly written erdos erdos either mistake typographical necessity\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the function defined above\n",
    "remove_stopwords(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(article, category):\n",
    "    \"\"\"\n",
    "    This function takes in a category and artical as a string. \n",
    "    Category must be an available category in inshorts, article is the link.\n",
    "    Returns a single inshort article.\n",
    "    \"\"\"\n",
    "    # Attribute selector\n",
    "    title = article.select(\"[itemprop='headline']\")[0].text\n",
    "    \n",
    "    # article body\n",
    "    content = article.select(\"[itemprop='articleBody']\")[0].text\n",
    "    \n",
    "    output = {}\n",
    "    output[\"title\"] = title\n",
    "    output[\"content\"] = content\n",
    "    output[\"category\"] = category\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_df(category):\n",
    "    \"\"\"\n",
    "    This function takes in a category as a string. Category must be an available category in inshorts\n",
    "    Returns a list of dictionaries where each dictionary represents a single inshort article\n",
    "    \"\"\"\n",
    "    base = \"https://inshorts.com/en/read/\"\n",
    "    \n",
    "    # We concatenate our base_url with the category\n",
    "    url = base + category\n",
    "    \n",
    "    # Set the headers to show as Netscape Navigator on Windows 98, b/c I feel like creating an anomaly in the logs\n",
    "    headers = {\"User-Agent\": \"Mozilla/4.5 (compatible; HTTrack 3.0x; Windows 98)\"}\n",
    "\n",
    "    # Get the http response object from the server\n",
    "    response = get(url, headers=headers)\n",
    "\n",
    "    # Make soup out of the raw html\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # Ignore everything, focusing only on the news cards\n",
    "    articles = soup.select(\".news-card\")\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    # Iterate through every article tag/soup \n",
    "    for article in articles:\n",
    "        \n",
    "        # Returns a dictionary of the article's title, body, and category\n",
    "        article_data = get_article(article, category) \n",
    "        \n",
    "        # Append the dictionary to the list\n",
    "        output.append(article_data)\n",
    "    \n",
    "    # Return the list of dictionaries\n",
    "    return pd.DataFrame(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kangana Ranaut's Twitter account suspended for...</td>\n",
       "      <td>Actress Kangana Ranaut's Twitter account has b...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter issues statement after permanently sus...</td>\n",
       "      <td>Twitter has permanently suspended Kangana Rana...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMS worm that tricks Android users into downlo...</td>\n",
       "      <td>A malware said to be targeting Android users i...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Melinda refuses spousal support from Bill Gate...</td>\n",
       "      <td>Melinda Gates officially filed for divorce fro...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple hires former Google AI scientist who lef...</td>\n",
       "      <td>Apple Inc said it has hired former Google scie...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accenture pledges ₹185 cr for COVID-19 relief ...</td>\n",
       "      <td>Global IT and professional services company Ac...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>US baseball team makes '1st' Dogecoin transact...</td>\n",
       "      <td>US baseball team Oakland Athletics President D...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Spotify CEO has contacted Arsenal owners for t...</td>\n",
       "      <td>Spotify CEO Daniel Ek has contacted Arsenal ow...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pro-Bitcoin messages projected on the walls of...</td>\n",
       "      <td>An unidentified cryptocurrency fan recently us...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sony strikes deal to integrate Discord in Play...</td>\n",
       "      <td>Sony has invested in gaming-focused chat app D...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Centre allows Airtel, Jio, Vodafone Idea, MTNL...</td>\n",
       "      <td>The Department of Telecommunications (DoT) has...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bitcoin is less volatile than Apple or Tesla s...</td>\n",
       "      <td>Cryptocurrency exchange platform Binance's CEO...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>S&amp;P Dow Jones launches 3 indices tied to Bitco...</td>\n",
       "      <td>The S&amp;P Dow Jones Indices said that it has lau...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Cryptocurrency market cap doubles to $2.3 tril...</td>\n",
       "      <td>The market value of cryptocurrencies reached a...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Meme-inspired cryptocurrency hits record high,...</td>\n",
       "      <td>Dogecoin, a cryptocurrency inspired by the Shi...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ether hits fresh record high above $3,400</td>\n",
       "      <td>Ether, the world's second-largest cryptocurren...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Apple executive suggested cutting App Store fe...</td>\n",
       "      <td>An Apple executive in charge of App Store sugg...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Twitter to acquire ad-free subscription platfo...</td>\n",
       "      <td>Twitter announced that it is acquiring Scroll ...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Users report delays in WazirX app after Dogeco...</td>\n",
       "      <td>Several users took to Twitter on Tuesday to re...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ola Electric hires ex-Aston Martin, Jaguar exe...</td>\n",
       "      <td>Ola Electric has announced the appointment of ...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wanted world to see Apple's control over all i...</td>\n",
       "      <td>Fortnite-maker Epic Games CEO Tim Sweeney on t...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Google cloud gaming service Stadia product hea...</td>\n",
       "      <td>Google's cloud gaming service Stadia VP and He...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>UK's Arrival, Uber to develop electric car for...</td>\n",
       "      <td>UK's technology company Arrival has partnered ...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sotheby's to accept cryptocurrency for auction...</td>\n",
       "      <td>Sotheby's will accept Bitcoin and Ethereum for...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Kangana Ranaut's Twitter account suspended for...   \n",
       "1   Twitter issues statement after permanently sus...   \n",
       "2   SMS worm that tricks Android users into downlo...   \n",
       "3   Melinda refuses spousal support from Bill Gate...   \n",
       "4   Apple hires former Google AI scientist who lef...   \n",
       "5   Accenture pledges ₹185 cr for COVID-19 relief ...   \n",
       "6   US baseball team makes '1st' Dogecoin transact...   \n",
       "7   Spotify CEO has contacted Arsenal owners for t...   \n",
       "8   Pro-Bitcoin messages projected on the walls of...   \n",
       "9   Sony strikes deal to integrate Discord in Play...   \n",
       "10  Centre allows Airtel, Jio, Vodafone Idea, MTNL...   \n",
       "11  Bitcoin is less volatile than Apple or Tesla s...   \n",
       "12  S&P Dow Jones launches 3 indices tied to Bitco...   \n",
       "13  Cryptocurrency market cap doubles to $2.3 tril...   \n",
       "14  Meme-inspired cryptocurrency hits record high,...   \n",
       "15          Ether hits fresh record high above $3,400   \n",
       "16  Apple executive suggested cutting App Store fe...   \n",
       "17  Twitter to acquire ad-free subscription platfo...   \n",
       "18  Users report delays in WazirX app after Dogeco...   \n",
       "19  Ola Electric hires ex-Aston Martin, Jaguar exe...   \n",
       "20  Wanted world to see Apple's control over all i...   \n",
       "21  Google cloud gaming service Stadia product hea...   \n",
       "22  UK's Arrival, Uber to develop electric car for...   \n",
       "23  Sotheby's to accept cryptocurrency for auction...   \n",
       "\n",
       "                                              content    category  \n",
       "0   Actress Kangana Ranaut's Twitter account has b...  technology  \n",
       "1   Twitter has permanently suspended Kangana Rana...  technology  \n",
       "2   A malware said to be targeting Android users i...  technology  \n",
       "3   Melinda Gates officially filed for divorce fro...  technology  \n",
       "4   Apple Inc said it has hired former Google scie...  technology  \n",
       "5   Global IT and professional services company Ac...  technology  \n",
       "6   US baseball team Oakland Athletics President D...  technology  \n",
       "7   Spotify CEO Daniel Ek has contacted Arsenal ow...  technology  \n",
       "8   An unidentified cryptocurrency fan recently us...  technology  \n",
       "9   Sony has invested in gaming-focused chat app D...  technology  \n",
       "10  The Department of Telecommunications (DoT) has...  technology  \n",
       "11  Cryptocurrency exchange platform Binance's CEO...  technology  \n",
       "12  The S&P Dow Jones Indices said that it has lau...  technology  \n",
       "13  The market value of cryptocurrencies reached a...  technology  \n",
       "14  Dogecoin, a cryptocurrency inspired by the Shi...  technology  \n",
       "15  Ether, the world's second-largest cryptocurren...  technology  \n",
       "16  An Apple executive in charge of App Store sugg...  technology  \n",
       "17  Twitter announced that it is acquiring Scroll ...  technology  \n",
       "18  Several users took to Twitter on Tuesday to re...  technology  \n",
       "19  Ola Electric has announced the appointment of ...  technology  \n",
       "20  Fortnite-maker Epic Games CEO Tim Sweeney on t...  technology  \n",
       "21  Google's cloud gaming service Stadia VP and He...  technology  \n",
       "22  UK's technology company Arrival has partnered ...  technology  \n",
       "23  Sotheby's will accept Bitcoin and Ethereum for...  technology  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df(\"technology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_codeup_blog(url):\n",
    "    \n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0\"}\n",
    "    response = get(url, headers = headers)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    # Title\n",
    "    Title = soup.select('h1.jupiterx-post-title')[0].text\n",
    "\n",
    "    # Body\n",
    "    Content = soup.select('.jupiterx-post-content')[0].text\n",
    "\n",
    "    # Time\n",
    "    Time = soup.time.text\n",
    "    \n",
    "    output = {}\n",
    "    output['Title'] = Title\n",
    "    output['Time'] = Time\n",
    "    output['Content'] = Content\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://codeup.com/codeups-data-science-career-accelerator-is-here/',\n",
    "        'https://codeup.com/data-science-myths/',\n",
    "        'https://codeup.com/data-science-vs-data-analytics-whats-the-difference/',\n",
    "        'https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/',\n",
    "        'https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeup_df(urls):\n",
    "    posts = [get_codeup_blog(url) for url in urls]\n",
    "    posts = pd.DataFrame(posts)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Time</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>September 30, 2018</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>October 31, 2018</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>October 17, 2018</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title                Time  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  September 30, 2018   \n",
       "1                                 Data Science Myths    October 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...    October 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair     August 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...     August 14, 2018   \n",
       "\n",
       "                                             Content  \n",
       "0  The rumors are true! The time has arrived. Cod...  \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...  \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...  \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...  \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = codeup_df(urls)\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) For each dataframe, produce the following columns:\n",
    "\n",
    "\n",
    "    - title to hold the title\n",
    "    - original to hold the original article/post content\n",
    "    - clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "    - stemmed to hold the stemmed version of the cleaned data.\n",
    "    - lemmatized to hold the lemmatized version of the cleaned data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(stem)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(lemmatize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-f69ae5571844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'clean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stemmed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_article_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'column' is not defined"
     ]
    }
   ],
   "source": [
    "df[['title', column,'clean', 'stemmed', 'lemmatized']] = prep_article_data(df, column, extra_words=[], exclude_words=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Ask yourself:\n",
    "\n",
    "\n",
    "   - If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "   - If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "   - If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It depends, if you have a lot of time use lemmmatized if not use stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
